# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (PyTorch –≤–µ—Ä—Å–∏—è —Å HuggingFace)
"""

from transformers import AutoTokenizer
import os


def create_or_load_tokenizer(vocab_path=None, model_name='cointegrated/rubert-tiny2'):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    
    Args:
        vocab_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        
    Returns:
        AutoTokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä HuggingFace
    """
    if vocab_path and os.path.exists(vocab_path):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ {vocab_path}")
        tokenizer = AutoTokenizer.from_pretrained(vocab_path)
    else:
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        print(f"üîß –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        special_tokens = {
            'pad_token': '[PAD]',
            'unk_token': '[UNK]',
            'bos_token': '[START]',
            'eos_token': '[END]'
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç
        tokens_to_add = {}
        for key, value in special_tokens.items():
            if getattr(tokenizer, key) is None:
                tokens_to_add[key] = value
        
        if tokens_to_add:
            tokenizer.add_special_tokens(tokens_to_add)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
        if vocab_path:
            os.makedirs(os.path.dirname(vocab_path), exist_ok=True)
            tokenizer.save_pretrained(vocab_path)
            print(f"üíæ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {vocab_path}")
    
    return tokenizer


def create_tokenizers(questions_vocab_path='data/vocab/tokenizer_qs', 
                     answers_vocab_path='data/vocab/tokenizer_an',
                     model_name='cointegrated/rubert-tiny2'):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤
    
    Args:
        questions_vocab_path: –ü—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É –≤–æ–ø—Ä–æ—Å–æ–≤
        answers_vocab_path: –ü—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É –æ—Ç–≤–µ—Ç–æ–≤
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        
    Returns:
        tuple: (tokenizer_qs, tokenizer_an)
    """
    print(f"\n{'='*70}")
    print("–°–û–ó–î–ê–ù–ò–ï –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–û–í")
    print(f"{'='*70}")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤
    tokenizer_qs = create_or_load_tokenizer(questions_vocab_path, model_name)
    print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ –≥–æ—Ç–æ–≤ (vocab size: {len(tokenizer_qs)})")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ)
    tokenizer_an = create_or_load_tokenizer(answers_vocab_path, model_name)
    print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ –≥–æ—Ç–æ–≤ (vocab size: {len(tokenizer_an)})")
    
    print(f"{'='*70}\n")
    
    return tokenizer_qs, tokenizer_an


def get_special_token_ids(tokenizer):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ ID —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    
    Args:
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        
    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å ID —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    """
    return {
        'pad_id': tokenizer.pad_token_id,
        'unk_id': tokenizer.unk_token_id,
        'start_id': tokenizer.bos_token_id if tokenizer.bos_token_id else tokenizer.cls_token_id,
        'end_id': tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.sep_token_id,
    }


def decode_tokens(tokenizer, token_ids, skip_special_tokens=True):
    """
    –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
    
    Args:
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        token_ids: ID —Ç–æ–∫–µ–Ω–æ–≤
        skip_special_tokens: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        
    Returns:
        str: –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    return tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)


def print_tokenizer_info(tokenizer, name="–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"):
    """
    –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ
    
    Args:
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    """
    print(f"\n{'='*70}")
    print(f"–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û {name.upper()}")
    print(f"{'='*70}")
    print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(tokenizer)}")
    print(f"PAD —Ç–æ–∫–µ–Ω: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    print(f"UNK —Ç–æ–∫–µ–Ω: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})")
    print(f"START —Ç–æ–∫–µ–Ω: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
    print(f"END —Ç–æ–∫–µ–Ω: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    print(f"{'='*70}\n")


def test_tokenizer(tokenizer, text="–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"):
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    
    Args:
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        text: –¢–µ–∫—Å—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    print(f"\n{'='*70}")
    print("–¢–ï–°–¢ –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê")
    print(f"{'='*70}")
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text}")
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    encoded = tokenizer(text, return_tensors='pt')
    print(f"–¢–æ–∫–µ–Ω—ã (IDs): {encoded['input_ids'][0].tolist()}")
    
    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    decoded = tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=True)
    print(f"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {decoded}")
    print(f"{'='*70}\n")
