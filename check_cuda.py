# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –∏ GPU –≤ PyTorch
"""

import torch
import sys


def check_cuda():
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU
    """
    print("=" * 70)
    print("–ü–†–û–í–ï–†–ö–ê CUDA –ò GPU –î–õ–Ø PYTORCH")
    print("=" * 70)
    
    # –í–µ—Ä—Å–∏—è PyTorch
    print(f"\nüì¶ PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
    cuda_available = torch.cuda.is_available()
    print(f"\nüî• CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {cuda_available}")
    
    if cuda_available:
        # –í–µ—Ä—Å–∏—è CUDA
        print(f"üîß CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU
        gpu_count = torch.cuda.device_count()
        print(f"üéÆ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {gpu_count}")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞–∂–¥–æ–º GPU
        print(f"\n{'=' * 70}")
        print("–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û GPU –£–°–¢–†–û–ô–°–¢–í–ê–•")
        print("=" * 70)
        
        for i in range(gpu_count):
            print(f"\nüñ•Ô∏è  GPU {i}:")
            print(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {torch.cuda.get_device_name(i)}")
            print(f"   Compute Capability: {torch.cuda.get_device_capability(i)}")
            
            # –ü–∞–º—è—Ç—å GPU
            total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"   –û–±—â–∞—è –ø–∞–º—è—Ç—å: {total_memory:.2f} GB")
            
            # –¢–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
            if torch.cuda.is_initialized():
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                reserved = torch.cuda.memory_reserved(i) / (1024**3)
                print(f"   –í—ã–¥–µ–ª–µ–Ω–æ –ø–∞–º—è—Ç–∏: {allocated:.2f} GB")
                print(f"   –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {reserved:.2f} GB")
        
        # –¢–µ–∫—É—â–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        current_device = torch.cuda.current_device()
        print(f"\nüéØ –¢–µ–∫—É—â–µ–µ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {current_device}")
        print(f"   ({torch.cuda.get_device_name(current_device)})")
        
        # –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–Ω–∑–æ—Ä–∞ –Ω–∞ GPU
        print(f"\n{'=' * 70}")
        print("–¢–ï–°–¢ –°–û–ó–î–ê–ù–ò–Ø –¢–ï–ù–ó–û–†–ê –ù–ê GPU")
        print("=" * 70)
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –Ω–∞ GPU
            test_tensor = torch.randn(1000, 1000).cuda()
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω —Ç–µ–Ω–∑–æ—Ä –Ω–∞ GPU")
            print(f"   –†–∞–∑–º–µ—Ä: {test_tensor.shape}")
            print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {test_tensor.device}")
            print(f"   –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {test_tensor.dtype}")
            
            # –ü—Ä–æ—Å—Ç–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –Ω–∞ GPU
            result = test_tensor @ test_tensor.T
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏—è —É–º–Ω–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü –Ω–∞ GPU")
            print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–∑–º–µ—Ä: {result.shape}")
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            del test_tensor, result
            torch.cuda.empty_cache()
            print(f"‚úÖ –ü–∞–º—è—Ç—å GPU –æ—á–∏—â–µ–Ω–∞")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å GPU: {e}")
            return False
        
        # cuDNN
        print(f"\n{'=' * 70}")
        cudnn_available = torch.backends.cudnn.is_available()
        print(f"üöÄ cuDNN –¥–æ—Å—Ç—É–ø–µ–Ω: {cudnn_available}")
        if cudnn_available:
            print(f"   cuDNN –≤–µ—Ä—Å–∏—è: {torch.backends.cudnn.version()}")
            print(f"   cuDNN enabled: {torch.backends.cudnn.enabled}")
        
        print(f"\n{'=' * 70}")
        print("‚úÖ –í–°–ï –ü–†–û–í–ï–†–ö–ò –ü–†–û–ô–î–ï–ù–´! GPU –ì–û–¢–û–í –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ")
        print("=" * 70)
        
        return True
        
    else:
        print(f"\n{'=' * 70}")
        print("‚ö†Ô∏è  CUDA –ù–ï –î–û–°–¢–£–ü–ù–ê")
        print("=" * 70)
        print("\n–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("1. NVIDIA GPU –¥—Ä–∞–π–≤–µ—Ä—ã –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
        print("2. PyTorch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –±–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ CUDA")
        print("3. –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è CUDA")
        print("\n–î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ PyTorch —Å CUDA 12.1:")
        print("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        print("=" * 70)
        
        return False


def get_recommended_device():
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device} ({torch.cuda.get_device_name(0)})")
    else:
        device = torch.device("cpu")
        print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    return device


if __name__ == "__main__":
    success = check_cuda()
    device = get_recommended_device()
    
    if not success and torch.cuda.is_available() == False:
        sys.exit(1)
    
    sys.exit(0)
