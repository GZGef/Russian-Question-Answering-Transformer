# -*- coding: utf-8 -*-
"""
–ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –Ω–∞ PyTorch —Å GPU
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.models.transformer import Transformer
from src.gpu_config import configure_gpu, print_device_info, print_memory_usage


class SimpleDialogueDataset(Dataset):
    """
    –ü—Ä–æ—Å—Ç–æ–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    """
    
    def __init__(self, num_samples=1000, max_len=20):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
            max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        self.num_samples = num_samples
        self.max_len = max_len
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        self.questions = torch.randint(3, 100, (num_samples, max_len))
        self.answers = torch.randint(3, 100, (num_samples, max_len))
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        return self.questions[idx], self.answers[idx]


def train_step(model, batch, criterion, optimizer, device):
    """
    –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
    
    Args:
        model: –ú–æ–¥–µ–ª—å
        batch: –ë–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö
        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        
    Returns:
        float: –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
    """
    questions, answers = batch
    questions = questions.to(device)
    answers = answers.to(device)
    
    # –í—Ö–æ–¥—ã –∏ —Ü–µ–ª–∏ –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞
    decoder_input = answers[:, :-1]  # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω
    targets = answers[:, 1:]  # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω
    
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    optimizer.zero_grad()
    logits = model(questions, decoder_input)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
    loss = criterion(
        logits.reshape(-1, logits.size(-1)),
        targets.reshape(-1)
    )
    
    # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
    loss.backward()
    optimizer.step()
    
    return loss.item()


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    """
    print("\n" + "="*70)
    print("–û–ë–£–ß–ï–ù–ò–ï –¢–†–ê–ù–°–§–û–†–ú–ï–†–ê –ù–ê PYTORCH –° GPU")
    print("="*70 + "\n")
    
    # 1. –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ
    print_device_info()
    
    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
    device = configure_gpu()
    
    # 3. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    print("\n" + "="*70)
    print("–ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò")
    print("="*70)
    
    num_layers = 2
    d_model = 128
    num_heads = 4
    dff = 256
    input_vocab_size = 1000
    target_vocab_size = 1000
    dropout_rate = 0.1
    
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: {num_layers}")
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {d_model}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: {num_heads}")
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å FFN: {dff}")
    print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {input_vocab_size}")
    print(f"Dropout: {dropout_rate}")
    
    # 4. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n" + "="*70)
    print("–°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("="*70)
    
    model = Transformer(
        num_layers=num_layers,
        d_model=d_model,
        num_heads=num_heads,
        dff=dff,
        input_vocab_size=input_vocab_size,
        target_vocab_size=target_vocab_size,
        dropout_rate=dropout_rate
    ).to(device)
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {num_params:,}")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # 5. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    print("\n" + "="*70)
    print("–°–û–ó–î–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê")
    print("="*70)
    
    dataset = SimpleDialogueDataset(num_samples=1000, max_len=20)
    dataloader = DataLoader(
        dataset,
        batch_size=32,
        shuffle=True,
        pin_memory=True if device.type == 'cuda' else False
    )
    
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω")
    print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(dataset)}")
    print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: 32")
    
    # 6. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    print("\n" + "="*70)
    print("–ù–ê–°–¢–†–û–ô–ö–ê –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*70)
    
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º padding
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam (lr=0.001)")
    print(f"‚úÖ –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å: CrossEntropyLoss")
    
    # 7. –û–±—É—á–µ–Ω–∏–µ
    print("\n" + "="*70)
    print("–û–ë–£–ß–ï–ù–ò–ï")
    print("="*70 + "\n")
    
    num_epochs = 3
    
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        
        for batch_idx, batch in enumerate(dataloader):
            loss = train_step(model, batch, criterion, optimizer, device)
            total_loss += loss
            
            if (batch_idx + 1) % 10 == 0:
                avg_loss = total_loss / (batch_idx + 1)
                print(f"–≠–ø–æ—Ö–∞ [{epoch+1}/{num_epochs}], "
                      f"–ë–∞—Ç—á [{batch_idx+1}/{len(dataloader)}], "
                      f"Loss: {avg_loss:.4f}")
        
        avg_epoch_loss = total_loss / len(dataloader)
        print(f"\n‚úÖ –≠–ø–æ—Ö–∞ {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω—è—è –ø–æ—Ç–µ—Ä—è: {avg_epoch_loss:.4f}\n")
    
    # 8. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ GPU
    if device.type == 'cuda':
        print_memory_usage()
    
    # 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("="*70)
    print("–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("="*70)
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'num_layers': num_layers,
        'd_model': d_model,
        'num_heads': num_heads,
        'dff': dff,
        'input_vocab_size': input_vocab_size,
        'target_vocab_size': target_vocab_size,
    }, 'transformer_pytorch.pth')
    
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ transformer_pytorch.pth")
    
    print("\n" + "="*70)
    print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("="*70 + "\n")
    
    print("üí° –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    print("   1. –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ GPU")
    print("   2. –î–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω—É–∂–Ω–æ:")
    print("      - –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–∏–∞–ª–æ–≥–æ–≤")
    print("      - –°–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å HuggingFace)")
    print("      - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è")
    print("   3. –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
    print("      model.generate(question_tokens, device=device)")


if __name__ == "__main__":
    main()