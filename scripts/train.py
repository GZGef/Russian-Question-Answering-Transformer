# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –Ω–∞ PyTorch
"""

import os
import sys
import gc
import time
import logging
import json

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm import tqdm
import matplotlib.pyplot as plt

from src.config import MAX_TOKENS, BATCH_SIZE, EPOCHS, LOGS_PATH, PLOTS_PATH
from src.models.transformer import Transformer
from src.data.dataset_pytorch import load_russian_dialogues, create_dataloader, print_sample_dialogues
from src.data.tokenizer_pytorch import create_tokenizers, print_tokenizer_info, get_special_token_ids
from src.gpu_config import configure_gpu, print_device_info, print_memory_usage, set_seed


def train_epoch(model, dataloader, criterion, optimizer, device, epoch, total_epochs):
    """
    –û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏
    
    Args:
        model: –ú–æ–¥–µ–ª—å
        dataloader: DataLoader
        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        epoch: –ù–æ–º–µ—Ä —ç–ø–æ—Ö–∏
        total_epochs: –í—Å–µ–≥–æ —ç–ø–æ—Ö
        
    Returns:
        float: –°—Ä–µ–¥–Ω—è—è –ø–æ—Ç–µ—Ä—è –∑–∞ —ç–ø–æ—Ö—É
    """
    model.train()
    total_loss = 0
    
    progress_bar = tqdm(dataloader, desc=f'–≠–ø–æ—Ö–∞ {epoch}/{total_epochs}')
    
    for batch_idx, batch in enumerate(progress_bar):
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        question_ids = batch['question_ids'].to(device)
        answer_ids = batch['answer_ids'].to(device)
        
        # –í—Ö–æ–¥—ã –∏ —Ü–µ–ª–∏ –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞
        decoder_input = answer_ids[:, :-1]  # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω
        targets = answer_ids[:, 1:]  # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω
        
        # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        optimizer.zero_grad()
        
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        logits = model(question_ids, decoder_input)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
        loss = criterion(
            logits.reshape(-1, logits.size(-1)),
            targets.reshape(-1)
        )
        
        # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
        loss.backward()
        
        # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_loss += loss.item()
        avg_loss = total_loss / (batch_idx + 1)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ progress bar
        progress_bar.set_postfix({'loss': f'{avg_loss:.4f}'})
    
    return total_loss / len(dataloader)


def validate(model, dataloader, criterion, device):
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    
    Args:
        model: –ú–æ–¥–µ–ª—å
        dataloader: DataLoader
        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        
    Returns:
        float: –°—Ä–µ–¥–Ω—è—è –ø–æ—Ç–µ—Ä—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for batch in dataloader:
            question_ids = batch['question_ids'].to(device)
            answer_ids = batch['answer_ids'].to(device)
            
            decoder_input = answer_ids[:, :-1]
            targets = answer_ids[:, 1:]
            
            logits = model(question_ids, decoder_input)
            
            loss = criterion(
                logits.reshape(-1, logits.size(-1)),
                targets.reshape(-1)
            )
            
            total_loss += loss.item()
    
    return total_loss / len(dataloader)


def save_checkpoint(model, optimizer, epoch, loss, filepath):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    
    Args:
        model: –ú–æ–¥–µ–ª—å
        optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        epoch: –ù–æ–º–µ—Ä —ç–ø–æ—Ö–∏
        loss: –ü–æ—Ç–µ—Ä—è
        filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
    """
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, filepath)
    print(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filepath}")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    """
    print("\n" + "="*70)
    print("–û–ë–£–ß–ï–ù–ò–ï –¢–†–ê–ù–°–§–û–†–ú–ï–†–ê –ù–ê PYTORCH –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò")
    print("="*70 + "\n")
    
    # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    os.makedirs(LOGS_PATH, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(os.path.join(LOGS_PATH, 'training.log'), encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)
    
    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    print_device_info()
    device = configure_gpu()
    set_seed(42)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ
    logger.info(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {EPOCHS}")
    logger.info(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {BATCH_SIZE}")
    logger.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {MAX_TOKENS}")
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset = load_russian_dialogues(max_samples=20000)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    print_sample_dialogues(dataset, num_samples=3)
    
    # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
    tokenizer_qs, tokenizer_an = create_tokenizers()
    print_tokenizer_info(tokenizer_qs, "–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤")
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Å–ª–æ–≤–∞—Ä–µ–π
    vocab_size_qs = len(tokenizer_qs)
    vocab_size_an = len(tokenizer_an)
    
    # 4. –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
    print(f"\n{'='*70}")
    print("–°–û–ó–î–ê–ù–ò–ï DATALOADER")
    print(f"{'='*70}")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train –∏ validation
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    
    train_dataset = dataset.select(range(train_size))
    val_dataset = dataset.select(range(train_size, len(dataset)))
    
    train_loader = create_dataloader(
        train_dataset, 
        tokenizer_qs, 
        tokenizer_an,
        batch_size=BATCH_SIZE,
        max_length=MAX_TOKENS,
        shuffle=True
    )
    
    val_loader = create_dataloader(
        val_dataset,
        tokenizer_qs,
        tokenizer_an,
        batch_size=BATCH_SIZE,
        max_length=MAX_TOKENS,
        shuffle=False
    )
    
    print(f"‚úÖ Train batches: {len(train_loader)}")
    print(f"‚úÖ Validation batches: {len(val_loader)}")
    print(f"{'='*70}\n")
    
    # 5. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print(f"{'='*70}")
    print("–°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
    print(f"{'='*70}")
    
    model = Transformer(
        num_layers=3,
        d_model=128,
        num_heads=4,
        dff=256,
        input_vocab_size=vocab_size_qs,
        target_vocab_size=vocab_size_an,
        dropout_rate=0.1
    ).to(device)
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {num_params:,}")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"{'='*70}\n")
    
    # 6. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_an.pad_token_id)
    optimizer = optim.Adam(model.parameters(), lr=0.0001)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    
    # 7. –û–±—É—á–µ–Ω–∏–µ
    print(f"{'='*70}")
    print("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    print(f"{'='*70}\n")
    
    # –°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    history = {
        'epoch': [],
        'train_loss': [],
        'val_loss': [],
        'learning_rate': []
    }
    
    best_val_loss = float('inf')
    
    for epoch in range(1, EPOCHS + 1):
        # –û–±—É—á–µ–Ω–∏–µ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch, EPOCHS)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_loss = validate(model, val_loader, criterion, device)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate
        scheduler.step(val_loss)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        history['epoch'].append(epoch)
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['learning_rate'].append(optimizer.param_groups[0]['lr'])
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.info(f"–≠–ø–æ—Ö–∞ {epoch}/{EPOCHS}:")
        logger.info(f"  Train Loss: {train_loss:.4f}")
        logger.info(f"  Val Loss: {val_loss:.4f}")
        logger.info(f"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        print(f"\n–≠–ø–æ—Ö–∞ {epoch}/{EPOCHS}:")
        print(f"  Train Loss: {train_loss:.4f}")
        print(f"  Val Loss: {val_loss:.4f}")
        print(f"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\n")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_checkpoint(
                model, optimizer, epoch, val_loss,
                'models/best_transformer_pytorch.pth'
            )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
        if epoch % 5 == 0:
            save_checkpoint(
                model, optimizer, epoch, val_loss,
                f'models/checkpoint_epoch_{epoch}.pth'
            )
    
    # 8. –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print(f"\n{'='*70}")
    print("–°–û–•–†–ê–ù–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò")
    print(f"{'='*70}")
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'vocab_size_qs': vocab_size_qs,
        'vocab_size_an': vocab_size_an,
        'num_layers': 3,
        'd_model': 128,
        'num_heads': 4,
        'dff': 256,
    }, 'models/final_transformer_pytorch.pth')
    
    print("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ models/final_transformer_pytorch.pth")
    
    # 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    print(f"\n{'='*70}")
    print("–°–û–•–†–ê–ù–ï–ù–ò–ï –ò–°–¢–û–†–ò–ò –û–ë–£–ß–ï–ù–ò–Ø –ò –ì–†–ê–§–ò–ö–û–í")
    print(f"{'='*70}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è –≤ JSON
    history_file = os.path.join(LOGS_PATH, 'training_history.json')
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {history_file}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    os.makedirs(PLOTS_PATH, exist_ok=True)
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å –æ–±—É—á–µ–Ω–∏—è
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(history['epoch'], history['train_loss'], 'b-', label='Train Loss', linewidth=2)
    plt.plot(history['epoch'], history['val_loss'], 'r-', label='Val Loss', linewidth=2)
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('–ü–æ—Ç–µ—Ä–∏')
    plt.title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(history['epoch'], history['learning_rate'], 'g-', linewidth=2)
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    loss_plot_path = os.path.join(PLOTS_PATH, 'training_loss.png')
    plt.savefig(loss_plot_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {loss_plot_path}")
    
    # –ì—Ä–∞—Ñ–∏–∫ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (—Ç–æ–ª—å–∫–æ –ø–æ—Ç–µ—Ä–∏)
    plt.figure(figsize=(10, 6))
    plt.plot(history['epoch'], history['train_loss'], 'b-', label='Train Loss', linewidth=2)
    plt.plot(history['epoch'], history['val_loss'], 'r-', label='Val Loss', linewidth=2)
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('–ü–æ—Ç–µ—Ä–∏')
    plt.title('–°—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    convergence_plot_path = os.path.join(PLOTS_PATH, 'convergence.png')
    plt.savefig(convergence_plot_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {convergence_plot_path}")
    
    # 10. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ GPU
    if device.type == 'cuda':
        print()
        print_memory_usage()
    
    print(f"\n{'='*70}")
    print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"{'='*70}\n")
    
    print("üí° –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    print("   1. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("   2. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ models/best_transformer_pytorch.pth")
    print("   3. –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏:")
    print("      - –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã")
    print("      - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ model.generate() –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤")
    print("   4. –î–ª—è –∑–∞–ø—É—Å–∫–∞ —á–∞—Ç-–±–æ—Ç–∞:")
    print("      python scripts/chat.py")


if __name__ == "__main__":
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
    os.makedirs('models', exist_ok=True)
    main()